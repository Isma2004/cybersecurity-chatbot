{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flask flask-cors pyngrok transformers torch accelerate -q\n!pip install sentencepiece protobuf -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:41:05.343251Z","iopub.execute_input":"2025-06-17T09:41:05.343524Z","iopub.status.idle":"2025-06-17T09:42:19.628335Z","shell.execute_reply.started":"2025-06-17T09:41:05.343503Z","shell.execute_reply":"2025-06-17T09:42:19.627504Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pyngrok import ngrok\nimport os\n\n# Set your ngrok authtoken (get free account at ngrok.com)\nNGROK_TOKEN = \"2yOxVP24BzxEg4jDNsXI98v3NSe_7qVyoCNChGE55kzLYwEyk\"  # Replace with your token\nngrok.set_auth_token(NGROK_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:42:19.630048Z","iopub.execute_input":"2025-06-17T09:42:19.630356Z","iopub.status.idle":"2025-06-17T09:42:20.900858Z","shell.execute_reply.started":"2025-06-17T09:42:19.630322Z","shell.execute_reply":"2025-06-17T09:42:20.900296Z"}},"outputs":[{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport threading\nimport json\nimport time\n\nprint(\"ğŸš€ Starting Mistral-7B API Server on Kaggle\")\nprint(f\"ğŸ® GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ“Š GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:42:20.901563Z","iopub.execute_input":"2025-06-17T09:42:20.901734Z","iopub.status.idle":"2025-06-17T09:42:48.359220Z","shell.execute_reply.started":"2025-06-17T09:42:20.901720Z","shell.execute_reply":"2025-06-17T09:42:48.358437Z"}},"outputs":[{"name":"stderr","text":"2025-06-17 09:42:34.848891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750153355.012371      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750153355.059061      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting Mistral-7B API Server on Kaggle\nğŸ® GPU Available: True\nğŸ“Š GPU Name: Tesla T4\nğŸ’¾ GPU Memory: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nprint(\"\\nğŸ“¥ Loading Mistral-7B-Instruct...\")\nprint(\"This will take 3-5 minutes on first run...\")\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# Option 1: Try to use environment variable for HF token if it exists\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\nhf_token = os.environ[\"HUGGINGFACE_TOKEN\"]\n\ntry:\n    # Load with optimization for GPU\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        load_in_8bit=False,  # Set True if memory issues\n        trust_remote_code=True,\n        token=hf_token  # Modern approach, replaces use_auth_token\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        token=hf_token\n    )\n\n    # Create pipeline\n    pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n\n    print(\"âœ… Mistral-7B loaded successfully!\")\n\nexcept Exception as e:\n    print(f\"âŒ Error loading Mistral: {e}\")\n    print(\"\\nTrying alternative approach with publicly available model...\")\n    \n    # Fallback to a public model if Mistral fails\n    fallback_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Example fallback\n    \n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            fallback_model,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n        \n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        print(f\"âœ… Fallback model {fallback_model} loaded successfully!\")\n    except Exception as e2:\n        print(f\"âŒ Fallback also failed: {e2}\")\n        print(\"\\nğŸ”‘ To use Mistral-7B, please login to Hugging Face:\")\n        print(\"    pip install huggingface-hub\")\n        print(\"    huggingface-cli login\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:42:48.360730Z","iopub.execute_input":"2025-06-17T09:42:48.361262Z","iopub.status.idle":"2025-06-17T09:44:04.143052Z","shell.execute_reply.started":"2025-06-17T09:42:48.361240Z","shell.execute_reply":"2025-06-17T09:44:04.142299Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“¥ Loading Mistral-7B-Instruct...\nThis will take 3-5 minutes on first run...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843b3ffa855f49fda0fb930be2131e79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f434ef1c3d54c7391b3c008f93a7020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42828217ef4f4760805d9675ac63cefe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ddc398064094240afd3d248268787bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ec2d623e62491eb796f8f9ae8b6458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67feca132648470a8ea9bf7afaec7653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54a2cf3a9ecc403c9186c7876f4fa4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"603cce64eac245f5a1d5066e54782b9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62fde5ad456542d8988d45eead8ba3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de9ff636aab40408a1b986509d3ff89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3ae08c6e2c4b65a4be5fde179b8c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b3bcf3006ea4afcaecc8b28f9710af3"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"âœ… Mistral-7B loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport torch\nimport time\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS for local access\n\n# Simple authentication\nAPI_KEY = \"your-secret-api-key-change-this\"\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        \"status\": \"healthy\",\n        \"model\": model_name,\n        \"gpu\": torch.cuda.is_available(),\n        \"device\": str(next(model.parameters()).device),\n        \"timestamp\": time.time()\n    })\n\n@app.route('/chat', methods=['POST'])\ndef chat():\n    # Check API key\n    auth_header = request.headers.get('Authorization')\n    if auth_header != f\"Bearer {API_KEY}\":\n        return jsonify({\"error\": \"Unauthorized\"}), 401\n    \n    try:\n        data = request.json\n        context = data.get('context', '')\n        question = data.get('question', '')\n        max_tokens = data.get('max_tokens', 512)\n        \n        # Create French cybersecurity prompt\n        prompt = f\"\"\"<s>[INST] Tu es un assistant expert en cybersÃ©curitÃ© et conformitÃ© ISO 27001.\n        \nContexte des documents:\n{context[:2000]}  # Limit context size\n\nQuestion: {question}\n\nRÃ©ponds en franÃ§ais professionnel en te basant UNIQUEMENT sur le contexte fourni. Si l'information n'est pas dans le contexte, dis-le clairement. [/INST]\"\"\"\n        \n        # Generate response\n        with torch.no_grad():\n            outputs = pipe(\n                prompt,\n                max_new_tokens=max_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                repetition_penalty=1.1,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        # Extract response\n        generated_text = outputs[0]['generated_text']\n        \n        # Remove prompt to get only response\n        if \"[/INST]\" in generated_text:\n            response_text = generated_text.split(\"[/INST]\")[-1].strip()\n        else:\n            response_text = generated_text[len(prompt):].strip()\n        \n        return jsonify({\n            \"response\": response_text,\n            \"model\": model_name,\n            \"tokens_used\": len(response_text.split())\n        })\n        \n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return jsonify({\"error\": str(e)}), 500\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:44:04.144060Z","iopub.execute_input":"2025-06-17T09:44:04.144318Z","iopub.status.idle":"2025-06-17T09:44:04.154532Z","shell.execute_reply.started":"2025-06-17T09:44:04.144298Z","shell.execute_reply":"2025-06-17T09:44:04.153959Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def run_server():\n    app.run(port=5000, threaded=True, use_reloader=False)\n\n# Start Flask in background thread\nserver_thread = threading.Thread(target=run_server)\nserver_thread.daemon = True\nserver_thread.start()\n\n# Give server time to start\ntime.sleep(5)\n\n# Create ngrok tunnel\npublic_url = ngrok.connect(5000)\nprint(\"\\n\" + \"=\"*50)\nprint(\"ğŸ‰ MISTRAL-7B API SERVER IS RUNNING!\")\nprint(\"=\"*50)\nprint(f\"ğŸŒ Public URL: {public_url}\")\nprint(f\"ğŸ”‘ API Key: {API_KEY}\")\nprint(f\"ğŸ“¡ Health Check: {public_url}/health\")\nprint(\"\\nğŸ’¾ Save these for your local app:\")\nprint(f\"KAGGLE_API_URL={public_url}\")\nprint(f\"KAGGLE_API_KEY={API_KEY}\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:44:04.155493Z","iopub.execute_input":"2025-06-17T09:44:04.155721Z","iopub.status.idle":"2025-06-17T09:44:09.747021Z","shell.execute_reply.started":"2025-06-17T09:44:04.155703Z","shell.execute_reply":"2025-06-17T09:44:09.746402Z"}},"outputs":[{"name":"stdout","text":" * Serving Flask app '__main__'\n * Debug mode: off\n\n==================================================\nğŸ‰ MISTRAL-7B API SERVER IS RUNNING!\n==================================================\nğŸŒ Public URL: NgrokTunnel: \"https://e8bd-34-9-121-98.ngrok-free.app\" -> \"http://localhost:5000\"\nğŸ”‘ API Key: your-secret-api-key-change-this\nğŸ“¡ Health Check: NgrokTunnel: \"https://e8bd-34-9-121-98.ngrok-free.app\" -> \"http://localhost:5000\"/health\n\nğŸ’¾ Save these for your local app:\nKAGGLE_API_URL=NgrokTunnel: \"https://e8bd-34-9-121-98.ngrok-free.app\" -> \"http://localhost:5000\"\nKAGGLE_API_KEY=your-secret-api-key-change-this\n==================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import requests\n\npublic_url = \"http://localhost:5000\"\nAPI_KEY=\"your-secret-api-key-change-this\"\n\n# Test health endpoint\ntry:\n    response = requests.get(f\"{public_url}/health\")\n    print(\"\\nâœ… Health check:\", response.json())\nexcept Exception as e:\n    print(f\"âŒ Health check failed: {e}\")\n\n# Test chat endpoint\ntest_request = {\n    \"context\": \"Les mots de passe doivent contenir au minimum 12 caractÃ¨res.\",\n    \"question\": \"Quelle est la longueur minimale des mots de passe?\",\n    \"max_tokens\": 100\n}\n\ntry:\n    response = requests.post(\n        f\"{public_url}/chat\",\n        json=test_request,\n        headers={\"Authorization\": f\"Bearer {API_KEY}\"}\n    )\n    print(\"\\nâœ… Chat test:\", response.json())\nexcept Exception as e:\n    print(f\"âŒ Chat test failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:44:09.747844Z","iopub.execute_input":"2025-06-17T09:44:09.748631Z","iopub.status.idle":"2025-06-17T09:44:13.161551Z","shell.execute_reply.started":"2025-06-17T09:44:09.748607Z","shell.execute_reply":"2025-06-17T09:44:13.160903Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Health check: {'device': 'cuda:0', 'gpu': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3', 'status': 'healthy', 'timestamp': 1750153449.7539222}\n\nâœ… Chat test: {'model': 'mistralai/Mistral-7B-Instruct-v0.3', 'response': 'La longueur minimale des mots de passe, selon le contexte fourni, doit Ãªtre de 12 caractÃ¨res.', 'tokens_used': 16}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"\\nâš ï¸ IMPORTANT: Keep this notebook running!\")\nprint(\"The server will stay active as long as the notebook is running.\")\nprint(\"Kaggle notebooks can run for up to 12 hours.\")\n\n# Keep notebook alive\nwhile True:\n    time.sleep(60)\n    print(f\"ğŸŸ¢ Server still running... {time.strftime('%Y-%m-%d %H:%M:%S')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:44:13.162345Z","iopub.execute_input":"2025-06-17T09:44:13.162548Z"}},"outputs":[{"name":"stdout","text":"\nâš ï¸ IMPORTANT: Keep this notebook running!\nThe server will stay active as long as the notebook is running.\nKaggle notebooks can run for up to 12 hours.\nğŸŸ¢ Server still running... 2025-06-17 09:45:13\nğŸŸ¢ Server still running... 2025-06-17 09:46:13\n","output_type":"stream"}],"execution_count":null}]}