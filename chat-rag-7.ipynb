{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\n\n# Suppress warnings for cleaner output\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"üîá Environment configured for clean output\")\n\n# Install packages\n!pip install flask flask-cors pyngrok transformers torch accelerate -q\n!pip install sentencepiece protobuf sentence-transformers -q\n\nprint(\"üì¶ Packages installed successfully\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\n\n# Set your ngrok token\nNGROK_TOKEN = \"..\"\nngrok.set_auth_token(NGROK_TOKEN)\n\nprint(\"üîó ngrok configured\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\nprint(\"üöÄ Starting Premium AI API Server\")\nprint(f\"üéÆ GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nprint(\"\\nüì• Loading Mistral-7B-Instruct...\")\n\n# Set up Hugging Face token for gated models\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"..\"\nhf_token = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n# Try Mistral models with authentication\nmodel_options = [\n    \"mistralai/Mistral-7B-Instruct-v0.3\",  # Latest version\n    \"mistralai/Mistral-7B-Instruct-v0.2\", \n    \"mistralai/Mistral-7B-Instruct-v0.1\"\n]\n\nmistral_loaded = False\nmodel = None\ntokenizer = None\npipe = None\n\nfor model_name in model_options:\n    try:\n        print(f\"üîÑ Trying {model_name} with HF token...\")\n        \n        # Load Mistral model with token\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            load_in_8bit=False,\n            trust_remote_code=True,\n            token=hf_token\n        )\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n        \n        # Fix missing pad token\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n\n        print(f\"‚úÖ {model_name} loaded successfully!\")\n        mistral_loaded = True\n        break\n        \n    except Exception as e:\n        print(f\"‚ùå {model_name} failed: {str(e)[:100]}...\")\n        continue\n\n# Fallback if all Mistral versions fail\nif not mistral_loaded:\n    print(\"\\nüîÑ Loading fallback model (no token required)...\")\n    try:\n        model_name = \"microsoft/DialoGPT-large\"\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            \n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        mistral_loaded = True\n        print(f\"‚úÖ Fallback model {model_name} loaded!\")\n    except Exception as e:\n        print(f\"‚ùå All models failed: {e}\")\n        mistral_loaded = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\nprint(\"\\nüì• Loading Embedding Model...\")\nembeddings_loaded = False\nembedding_model = None\n\ntry:\n    embedding_model = SentenceTransformer('intfloat/multilingual-e5-base')\n    print(\"‚úÖ intfloat/multilingual-e5-base loaded successfully!\")\n    embeddings_loaded = True\nexcept Exception as e:\n    print(f\"‚ùå Embedding model failed: {e}\")\n    try:\n        # Fallback embedding model\n        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"‚úÖ Fallback embedding model loaded!\")\n        embeddings_loaded = True\n    except Exception as e2:\n        print(f\"‚ùå All embedding models failed: {e2}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport threading\nimport time\n\napp = Flask(__name__)\nCORS(app)\nAPI_KEY = \"..\"\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        \"status\": \"healthy\",\n        \"model\": model_name if mistral_loaded else \"none\",\n        \"gpu\": torch.cuda.is_available(),\n        \"device\": str(next(model.parameters()).device) if mistral_loaded else \"cpu\",\n        \"timestamp\": time.time(),\n        \"models_loaded\": {\n            \"mistral\": mistral_loaded,\n            \"embeddings\": embeddings_loaded\n        },\n        \"models\": {\n            \"chat\": model_name if mistral_loaded else \"none\",\n            \"embeddings\": \"intfloat/multilingual-e5-base\" if embeddings_loaded else \"none\"\n        }\n    })\n\n@app.route('/chat', methods=['POST'])\ndef chat():\n    auth_header = request.headers.get('Authorization')\n    if auth_header != f\"Bearer {API_KEY}\":\n        return jsonify({\"error\": \"Unauthorized\"}), 401\n    \n    if not mistral_loaded:\n        return jsonify({\"error\": \"Chat model not loaded\"}), 503\n    \n    try:\n        data = request.json\n        context = data.get('context', '')\n        question = data.get('question', '')\n        max_tokens = data.get('max_tokens', 512)\n        \n        # Create optimized prompt\n        if \"mistral\" in model_name.lower():\n            prompt = f\"\"\"<s>[INST] Tu es CyberSense, un assistant expert senior en cybers√©curit√© sp√©cialis√© dans les  environnements bancaires et d'entreprise.\n            Tu travailles pour Cr√©dit Agricole du Maroc en tant que consultant en s√©curit√© informatique et conformit√© r√©glementaire.\n\n  ## TON EXPERTISE:\n  ‚Ä¢ Expert certifi√© en cybers√©curit√© avec 15+ ann√©es d'exp√©rience\n  ‚Ä¢ Sp√©cialiste des normes ISO 27001/27002, ISO 22301, et NIST Cybersecurity Framework\n  ‚Ä¢ Expert en conformit√© bancaire (Directives PCI DSS, B√¢le III, GDPR/RGPD)\n  ‚Ä¢ Consultant en gestion des risques cyber pour institutions financi√®res\n  ‚Ä¢ Sp√©cialiste en architecture de s√©curit√© d'entreprise et gouvernance IT\n\n  ## TES DOMAINES DE COMP√âTENCE:\n  ‚Ä¢ S√©curit√© des syst√®mes d'information bancaires et financiers\n  ‚Ä¢ Gestion des identit√©s et acc√®s (IAM) en environnement d'entreprise\n  ‚Ä¢ Cryptographie appliqu√©e et protection des donn√©es sensibles\n  ‚Ä¢ D√©tection et r√©ponse aux incidents de s√©curit√© (SOC/SIEM)\n  ‚Ä¢ Audit de s√©curit√© et tests d'intrusion en milieu bancaire\n  ‚Ä¢ Conformit√© r√©glementaire (Bank Al-Maghrib, GDPR, ISO 27001)\n  ‚Ä¢ Formation et sensibilisation du personnel aux risques cyber\n\n  ## CONTEXTE DOCUMENTAIRE:\n  {context[:2000]}\n\n  ## QUESTION √Ä TRAITER:\n  {question}\n\n  ## INSTRUCTIONS STRICTES:\n  1. **R√©ponse en fran√ßais professionnel uniquement** - Utilise un vocabulaire technique pr√©cis et un ton expert\n  2. **Base-toi EXCLUSIVEMENT sur le contexte fourni** - Ne jamais inventer ou supposer des informations\n  3. **Cite syst√©matiquement tes sources** - R√©f√©rence les documents utilis√©s avec pr√©cision\n  4. **Respecte la confidentialit√© bancaire** - Aucune information sensible ne doit √™tre divulgu√©e\n  5. **Applique le principe de pr√©caution** - En cas de doute, recommande une approche s√©curis√©e\n  6. **Structure tes r√©ponses clairement** - Utilise des listes, sections et priorit√©s quand pertinent\n  7. **Si l'information n'est pas dans le contexte** - Indique clairement \"Selon les documents fournis, cette information n'est pas disponible\"\n  8. **Recommande toujours les meilleures pratiques** -  Propose des mesures concr√®tes et applicables\n  9. **Consid√®re l'environnement bancaire** - Prends en compte les sp√©cificit√©s du secteur financier\n  10. **Respecte les r√©glementations en vigueur** - Assure la conformit√© avec les standards internationaux\n\n  ## FORMATAGE DE R√âPONSE:\n  ‚Ä¢ Commence par un r√©sum√© ex√©cutif si la question est complexe\n  ‚Ä¢ Utilise des puces pour les listes d'actions ou recommandations  \n  ‚Ä¢ Indique le niveau de criticit√© (Faible/Moyen/√âlev√©/Critique) si applicable\n  ‚Ä¢ Conclus par les prochaines √©tapes recommand√©es si pertinent\n\n  R√©ponds maintenant en tant qu'expert cybers√©curit√© de Cr√©dit Agricole du Maroc: [/INST]\"\"\"\n        else:\n            prompt = f\"\"\"Contexte: {context[:1500]}\n\nQuestion: {question}\n\nR√©ponse en fran√ßais:\"\"\"\n        \n        # Generate response\n        with torch.no_grad():\n            outputs = pipe(\n                prompt,\n                max_new_tokens=max_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                repetition_penalty=1.1,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        generated_text = outputs[0]['generated_text']\n        \n        # Extract response\n        if \"[/INST]\" in generated_text:\n            response_text = generated_text.split(\"[/INST]\")[-1].strip()\n        else:\n            response_text = generated_text[len(prompt):].strip()\n        \n        return jsonify({\n            \"response\": response_text,\n            \"model\": model_name,\n            \"tokens_used\": len(response_text.split())\n        })\n        \n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/embeddings', methods=['POST'])\ndef embeddings():\n    auth_header = request.headers.get('Authorization')\n    if auth_header != f\"Bearer {API_KEY}\":\n        return jsonify({\"error\": \"Unauthorized\"}), 401\n    \n    if not embeddings_loaded:\n        return jsonify({\"error\": \"Embedding model not loaded\"}), 503\n    \n    try:\n        data = request.json\n        texts = data.get('texts', [])\n        is_query = data.get('is_query', False)\n        \n        if not texts:\n            return jsonify({\"error\": \"No texts provided\"}), 400\n        \n        # Add prefixes for better embeddings\n        if is_query:\n            prefixed_texts = [f\"query: {text}\" for text in texts]\n        else:\n            prefixed_texts = [f\"passage: {text}\" for text in texts]\n        \n        # Generate embeddings\n        embeddings = embedding_model.encode(prefixed_texts, convert_to_numpy=True)\n        embeddings_list = [emb.tolist() for emb in embeddings]\n        \n        return jsonify({\n            \"embeddings\": embeddings_list,\n            \"model\": \"intfloat/multilingual-e5-base\",\n            \"dimension\": len(embeddings_list[0]) if embeddings_list else 0\n        })\n        \n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_server():\n    app.run(port=5000, threaded=True, use_reloader=False)\n\n# Start Flask server\nserver_thread = threading.Thread(target=run_server)\nserver_thread.daemon = True\nserver_thread.start()\n\ntime.sleep(5)\n\n# Create ngrok tunnel and extract URL properly\ntunnel = ngrok.connect(5000)\npublic_url = tunnel.public_url  # Extract string URL\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ PREMIUM AI API SERVER IS RUNNING!\")\nprint(\"=\"*60)\nprint(f\"üåê Public URL: {public_url}\")\nprint(f\"üîë API Key: {API_KEY}\")\nprint(f\"üì° Health: {public_url}/health\")\nprint(f\"üí¨ Chat: {public_url}/chat\")\nprint(f\"üßÆ Embeddings: {public_url}/embeddings\")\nprint(f\"\\nüíæ For your .env file:\")\nprint(f\"KAGGLE_API_URL={public_url}\")\nprint(f\"KAGGLE_API_KEY={API_KEY}\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\nprint(\"\\nüß™ Testing all endpoints...\")\n\n# Test health\ntry:\n    response = requests.get(f\"{public_url}/health\")\n    health_data = response.json()\n    print(f\"‚úÖ Health: {health_data['status']}\")\n    print(f\"   Chat model: {health_data['models']['chat']}\")\n    print(f\"   Embedding model: {health_data['models']['embeddings']}\")\nexcept Exception as e:\n    print(f\"‚ùå Health failed: {e}\")\n\n# Test chat\ntry:\n    test_chat = {\n        \"context\": \"Les mots de passe doivent contenir au minimum 12 caract√®res avec des majuscules, minuscules, chiffres et symboles.\",\n        \"question\": \"Quelle est la politique des mots de passe?\",\n        \"max_tokens\": 150\n    }\n    \n    response = requests.post(\n        f\"{public_url}/chat\",\n        json=test_chat,\n        headers={\"Authorization\": f\"Bearer {API_KEY}\"}\n    )\n    result = response.json()\n    print(f\"‚úÖ Chat: {result['response'][:100]}...\")\n    print(f\"   Tokens: {result['tokens_used']}\")\nexcept Exception as e:\n    print(f\"‚ùå Chat failed: {e}\")\n\n# Test embeddings\ntry:\n    test_embeddings = {\n        \"texts\": [\"cybers√©curit√© et protection des donn√©es\", \"politique de mots de passe\"],\n        \"is_query\": False\n    }\n    \n    response = requests.post(\n        f\"{public_url}/embeddings\",\n        json=test_embeddings,\n        headers={\"Authorization\": f\"Bearer {API_KEY}\"}\n    )\n    result = response.json()\n    print(f\"‚úÖ Embeddings: {len(result['embeddings'])} vectors generated\")\n    print(f\"   Dimension: {result['dimension']}\")\nexcept Exception as e:\n    print(f\"‚ùå Embeddings failed: {e}\")\n\nprint(\"\\nüéØ All systems ready! Copy the URL to your .env file.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n‚ö†Ô∏è IMPORTANT: Keep this notebook running!\")\nprint(\"Server stays active while notebook runs (up to 12 hours)\")\n\nwhile True:\n    time.sleep(60)\n    print(f\"üü¢ Server running... {time.strftime('%H:%M:%S')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}